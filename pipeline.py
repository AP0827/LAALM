"""
MVP: Multi-Modal Transcription with Word-Level Confidence and Groq Semantic Correction

This script combines:
1. DeepGram - Audio transcription with word-level confidence
2. auto_avsr - Visual speech recognition with word-level confidence  
3. Groq - Fast LLM-based semantic correction

Perfect for scenarios where you have both audio and video input.
"""

import os
import sys
import numpy as np
from typing import Dict, Any, List, Tuple, Optional

# Load environment variables from .env file
from load_env import load_env_file
load_env_file()

# Import logger
from logger import get_logger

# Add project paths
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'auto_avsr'))
sys.path.insert(0, os.path.dirname(__file__))


def get_deepgram_confidence(audio_file: str, api_key: Optional[str] = None) -> Dict[str, Any]:
    """
    Get transcription with word-level confidence from DeepGram.
    
    Args:
        audio_file: Path to audio file
        api_key: DeepGram API key (uses DEEPGRAM_API_KEY env var if not provided)
        
    Returns:
        Dict with transcript, overall_confidence, and word_confidences
    """
    if api_key is None:
        api_key = os.getenv("DEEPGRAM_API_KEY")
    
    if not api_key:
        raise ValueError("DEEPGRAM_API_KEY environment variable not set")
    
    try:
        from DeepGram.enhanced_transcriber import DeepGramWithConfidence
        
        print("üìä Transcribing audio with DeepGram...")
        transcriber = DeepGramWithConfidence(api_key=api_key)
        result = transcriber.transcribe_file_with_confidence(audio_file)
        
        print(f"   ‚úì DeepGram transcript: {result['transcript']}")
        print(f"   ‚úì Confidence: {result['overall_confidence']:.3f}")
        print(f"   ‚úì Word-level scores: {result['word_confidences']}")
        
        return result
    
    except Exception as e:
        print(f"   ‚úó DeepGram error: {e}")
        # Return mock data for demo
        return {
            "transcript": "the quick brown fox jumps over the lazy dog",
            "overall_confidence": 0.92,
            "word_confidences": [
                ("the", 0.95), ("quick", 0.89), ("brown", 0.91),
                ("fox", 0.93), ("jumps", 0.87), ("over", 0.94),
                ("the", 0.96), ("lazy", 0.88), ("dog", 0.97)
            ]
        }


def get_avsr_confidence(video_file: str, model_path: Optional[str] = None) -> Dict[str, Any]:
    """
    Get transcription with word-level confidence from auto_avsr.
    
    Args:
        video_file: Path to video file
        model_path: Path to auto_avsr model weights (uses default if not provided)
        
    Returns:
        Dict with transcript, overall_confidence, and word_confidences
    """
    if model_path is None:
        # Try to find model weights automatically
        default_models = [
            "auto_avsr/pretrained_models/vsr_trlrs2lrs3vox2avsp_base.pth",
            "./auto_avsr/pretrained_models/vsr_trlrs2lrs3vox2avsp_base.pth",
            os.path.join(os.path.dirname(__file__), "auto_avsr/pretrained_models/vsr_trlrs2lrs3vox2avsp_base.pth"),
        ]
        
        for path in default_models:
            if os.path.exists(path):
                model_path = path
                break
    
    if not model_path or not os.path.exists(model_path):
        print(f"   ‚úó auto_avsr model not found. Tried: {default_models}")
        # Return mock data for demo
        return {
            "transcript": "the quick brown fox jumps over the lazy dog",
            "overall_confidence": 0.88,
            "word_confidences": [
                ("the", 0.92), ("quick", 0.85), ("brown", 0.88),
                ("fox", 0.90), ("jumps", 0.83), ("over", 0.91),
                ("the", 0.94), ("lazy", 0.82), ("dog", 0.95)
            ]
        }
    
    try:
        # Import from auto_avsr
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'auto_avsr'))
        from inference_wrapper import get_avsr_confidence as avsr_inference
        
        print("üëÅÔ∏èüëÑ Transcribing video with auto_avsr (VSR)...")
        result = avsr_inference(video_file, model_path, detector="retinaface")
        
        print(f"   ‚úì auto_avsr transcript: {result['transcript']}")
        print(f"   ‚úì Confidence: {result['overall_confidence']:.3f}")
        print(f"   ‚úì Word-level scores: {result['word_confidences']}")
        
        return {
            "transcript": result['transcript'],
            "overall_confidence": result['overall_confidence'],
            "word_confidences": result['word_confidences'],
        }
    
    except Exception as e:
        print(f"   ‚úó auto_avsr error: {e}")
        import traceback
        traceback.print_exc()
        # Return mock data for demo
        return {
            "transcript": "the quick brown fox jumps over the lazy dog",
            "overall_confidence": 0.88,
            "word_confidences": [
                ("the", 0.92), ("quick", 0.85), ("brown", 0.88),
                ("fox", 0.90), ("jumps", 0.83), ("over", 0.91),
                ("the", 0.94), ("lazy", 0.82), ("dog", 0.95)
            ]
        }


def combine_word_confidences(
    deepgram_words: List[Tuple[str, float]],
    avsr_words: List[Tuple[str, float]],
) -> List[Dict[str, Any]]:
    """
    Combine word-level confidence scores using ROBUST ALIGNMENT and Attention Fusion.
    
    IMPROVEMENT: Now uses Needleman-Wunsch alignment to align audio and visual
    streams before fusion, preventing cascading errors from missed words.
    
    Args:
        deepgram_words: List of tuples from DeepGram. Can be (word, conf) or (word, conf, start, end).
        avsr_words: List of tuples from auto_avsr. (word, conf).
        
    Returns:
        List of dicts with combined confidence information + attention weights
    """
    # Import our modules
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'Transformer'))
    from attention_fusion import AttentionFusion
    from alignment import TranscriptAligner
    
    # 1. Initialize Aligner
    aligner = TranscriptAligner(
        match_score=2.0,
        mismatch_penalty=-1.0,
        gap_penalty=-1.0
    )
    
    # 2. Calibration (UNBIASING)
    # The user requested a "more unbiased" fusion.
    # Audio confidence is typically higher (0.8-0.9) than Visual (0.5-0.6).
    # We calibrate Visual scores to match Audio distribution.
    
    audio_confs = [w[1] for w in deepgram_words]
    video_confs = [w[1] for w in avsr_words]
    
    if audio_confs and video_confs:
        a_mean, a_std = np.mean(audio_confs), np.std(audio_confs)
        v_mean, v_std = np.mean(video_confs), np.std(video_confs)
        
        # Avoid division by zero
        v_std = v_std if v_std > 0.01 else 0.1
        a_std = a_std if a_std > 0.01 else 0.1
        
        print(f"‚öñÔ∏è  Calibration: Audio Œº={a_mean:.2f} œÉ={a_std:.2f} | Visual Œº={v_mean:.2f} œÉ={v_std:.2f}")
        
        # Scale visual words
        calibrated_avsr_words = []
        for w, conf in avsr_words:
            # Z-score normalization + re-scaling to Audio distribution
            z_score = (conf - v_mean) / v_std
            new_conf = (z_score * a_std) + a_mean
            # Clip to [0, 1]
            new_conf = max(0.01, min(0.99, new_conf))
            calibrated_avsr_words.append((w, new_conf))
        
        # Use calibrated words for alignment and fusion
        avsr_words_for_fusion = calibrated_avsr_words
        print("   ‚úì Visual confidence calibrated to match audio distribution")
    else:
        avsr_words_for_fusion = avsr_words

    # 3. Perform Robust Alignment
    # Align the sequences (returns lists of same length with None for gaps)
    aligned_audio, aligned_visual = aligner.align(deepgram_words, avsr_words_for_fusion)
    
    # 4. Prepare for Fusion
    # We need to convert the aligned lists (which may have None) back into 
    # the format expected by AttentionFusion, but strictly paired.
    
    paired_audio = []
    paired_visual = []
    
    for a, v in zip(aligned_audio, aligned_visual):
        # Handle Audio Gap (Visual Only)
        if a is None:
            paired_audio.append(("", 0.0))
        else:
            # Handle both 2-tuple and 4-tuple formats
            paired_audio.append((a[0], a[1]))
            
        # Handle Visual Gap (Audio Only)
        if v is None:
            paired_visual.append(("", 0.0))
        else:
            paired_visual.append((v[0], v[1]))
            
    # 3. Perform Attention-Based Fusion
    fusion = AttentionFusion(
        temperature=2.0,           
        context_window=3,          
        switching_penalty=0.15,    
        min_confidence_threshold=0.6
    )
    
    fusion_result = fusion.fuse_transcripts(
        audio_words=paired_audio,
        visual_words=paired_visual
    )
    
    # 4. Format Output
    combined = []
    for i, detail in enumerate(fusion_result.word_details):
        # Recover original metadata if available (timestamps from audio)
        audio_source = aligned_audio[i]
        start_time = 0.0
        end_time = 0.0
        
        if audio_source and len(audio_source) >= 4:
            start_time = audio_source[2]
            end_time = audio_source[3]
            
        combined.append({
            "position": detail['position'],
            "word": detail['word'],
            "deepgram": {
                "word": detail['audio_word'], 
                "confidence": detail['audio_conf']
            },
            "avsr": {
                "word": detail['visual_word'], 
                "confidence": detail['visual_conf']
            },
            "average_confidence": detail['confidence'],
            "agreement": detail['audio_word'].lower() == detail['visual_word'].lower(),
            "low_confidence": detail['confidence'] < 0.6,
            "selected_modality": detail['selected_modality'],
            "audio_weight": detail['audio_weight'],
            "visual_weight": detail['visual_weight'],
            "start": start_time,
            "end": end_time
        })
    
    return combined


def process_with_groq(
    deepgram_transcript: str,
    deepgram_confidence: float,
    deepgram_words: List[Tuple[str, float]],
    avsr_transcript: str,
    avsr_confidence: float,
    avsr_words: List[Tuple[str, float]],
    combined_words: List[Dict[str, Any]],
    api_key: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Use Groq to semantically correct and refine the combined output.
    
    Args:
        deepgram_transcript: Full transcript from DeepGram
        deepgram_confidence: Overall confidence from DeepGram
        deepgram_words: Word-level confidences from DeepGram
        avsr_transcript: Full transcript from auto_avsr
        avsr_confidence: Overall confidence from auto_avsr
        avsr_words: Word-level confidences from auto_avsr
        combined_words: Combined word-level analysis
        api_key: Groq API key (uses GROQ_API_KEY env var if not provided)
        
    Returns:
        Dict with corrected transcript and metadata
    """
    if api_key is None:
        api_key = os.getenv("GROQ_API_KEY")
    
    if not api_key:
        print("   ‚ö† GROQ_API_KEY not set - using fallback mode")
        # Return the higher-confidence transcript as fallback
        return {
            "corrected_transcript": (
                deepgram_transcript if deepgram_confidence >= avsr_confidence 
                else avsr_transcript
            ),
            "corrections": [],
            "confidence": max(deepgram_confidence, avsr_confidence),
            "notes": "Groq API key not set, returning higher-confidence modality",
            "status": "fallback",
        }
    
    try:
        from groq import Groq
        
        client = Groq(api_key=api_key)
        
        # Build context for Groq
        low_conf_words = [w for w in combined_words if w["low_confidence"]]
        disagreements = [w for w in combined_words if not w["agreement"]]
        
        prompt = f"""You are an expert speech transcription refinement system with deep understanding of linguistics, phonetics, and semantic coherence.

I have two transcriptions from different modalities (audio ASR and visual lip-reading) of the same speech:

AUDIO TRANSCRIPTION (DeepGram - reliable in clean conditions):
- Transcript: "{deepgram_transcript}"
- Overall Confidence: {deepgram_confidence:.3f}
- Word Confidences: {deepgram_words}

VISUAL TRANSCRIPTION (auto_avsr - lip reading, prone to homophones):
- Transcript: "{avsr_transcript}"
- Overall Confidence: {avsr_confidence:.3f}
- Word Confidences: {avsr_words}

COMBINED ANALYSIS:
- Total Words: {len(combined_words)}
- Low Confidence Words (< 0.7): {len(low_conf_words)}
   {[w['word'] for w in low_conf_words]}
- Word Disagreements: {len(disagreements)}
  {disagreements}

CRITICAL INSTRUCTIONS:
1. **TRUST THE FUSION LAYER** - The combined transcript has already intelligently selected words based on:
   - Confidence scores (calibrated across modalities)
   - Phonetic analysis (viseme-aware disambiguation)
   - Context consistency
   - Alignment quality
   
2. **YOUR ROLE IS SEMANTIC REFINEMENT ONLY** - Focus on:
   - Grammar and sentence structure
   - Contextual plausibility
   - Fixing obvious transcription artifacts (merged words, punctuation)
   - Validating the combined transcript makes sense
   
3. **DO NOT second-guess modality selection** - The fusion already chose the best word for each position

4. **Common transcription artifacts to fix:**
   - Merged words (e.g., "binblue" ‚Üí "bin blue", "f two" ‚Üí "f two" is correct)
   - Missing punctuation
   - Capitalization inconsistencies
   - Obvious grammar errors

5. **LRS3 Grid Commands Context:**
   These command patterns are VALID and common in the dataset:
   - "[action] [color] [with/at] [letter] [number] [timing]"
   - Examples: "lay white with zed nine soon", "bin blue at f two please"
   - Colors: red, blue, white, green, etc.
   - Letters: a-z (spelled out or single)
   - Numbers: 1-10
   - Actions: lay, bin, set, place, put
   - Timing: now, soon, please, again

DECISION PROCESS:
Step 1: Check if the combined transcript is semantically coherent
Step 2: If it makes sense (including LRS3 command patterns), accept it with minimal changes
Step 3: Only make corrections for clear grammar/semantic issues, NOT modality preference
Step 4: Preserve the word choices from the fusion layer unless they create nonsensical phrases

Respond with ONLY a JSON object (no markdown, no code blocks):
{{
  "corrected_transcript": "the refined transcript",
  "corrections": [
    {{"original_phrase": "...", "corrected_phrase": "...", "reason": "..."}},
  ],
  "confidence_score": 0.85,
  "status": "success",
  "notes": "brief explanation of correction strategy"
}}"""
        
        print("\nüß† Sending to Groq for semantic correction...")
        
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert transcription refinement system with deep linguistic knowledge. Always prioritize semantic coherence and grammatical correctness. Respond with valid JSON only.",
                },
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            temperature=0.3,
            max_tokens=1000,
        )
        
        result_text = response.choices[0].message.content
        
        # Parse JSON response
        import json
        result_json = json.loads(result_text)
        
        return {
            "corrected_transcript": result_json.get("corrected_transcript", ""),
            "corrections": result_json.get("corrections", []),
            "confidence": float(result_json.get("confidence_score", 0.5)),
            "notes": result_json.get("notes", ""),
            "status": "success",
        }
    
    except Exception as e:
        print(f"   ‚úó Groq error: {e}")
        # Return the higher-confidence transcript as fallback
        return {
            "corrected_transcript": (
                deepgram_transcript if deepgram_confidence >= avsr_confidence 
                else avsr_transcript
            ),
            "corrections": [],
            "confidence": max(deepgram_confidence, avsr_confidence),
            "notes": f"Groq failed, returning higher-confidence modality. Error: {str(e)}",
            "status": "fallback",
        }


def run_mvp(
    audio_file: Optional[str] = None,
    video_file: Optional[str] = None,
    deepgram_api_key: Optional[str] = None,
    groq_api_key: Optional[str] = None,
    avsr_model_path: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Run the complete MVP pipeline.
    
    Args:
        audio_file: Path to audio file (required if no mock mode)
        video_file: Path to video file (required if no mock mode)
        deepgram_api_key: DeepGram API key
        groq_api_key: Groq API key
        avsr_model_path: Path to auto_avsr model weights
        
    Returns:
        Complete results including corrected transcript
    """
    print("\n" + "=" * 80)
    print(" MVP: Multi-Modal Transcription with Groq Semantic Correction")
    print("=" * 80)
    
    # Get audio transcription with confidence
    processed_audio_path = None
    if audio_file and os.path.exists(audio_file):
        # We process sending the ORIGINAL audio file to DeepGram.
        # Previous experimentation with 'scipy' re-encoding caused quality degradation.
        # DeepGram handles raw audio formats better than our simple preprocessor.
        print("üìä Transcribing audio with DeepGram (Direct)...")
        deepgram_result = get_deepgram_confidence(audio_file, deepgram_api_key)
    else:
        print("üìä Using mock DeepGram data (no audio file provided)")
        deepgram_result = {
            "transcript": "the quick brown fox jumps over the lazy dog",
            "overall_confidence": 0.92,
            "word_confidences": [
                ("the", 0.95), ("quick", 0.89), ("brown", 0.91),
                ("fox", 0.93), ("jumps", 0.87), ("over", 0.94),
                ("the", 0.96), ("lazy", 0.88), ("dog", 0.97)
            ]
        }
    
    # Get video transcription with confidence
    processed_video_path = None
    if video_file and os.path.exists(video_file):
        try:
            # Preprocess video (CLAHE & Sharpening for better VSR)
            print("üé• Preprocessing video (enhancing frames)...")
            sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'auto_avsr'))
            from preprocessor import VideoPreprocessor
            
            preprocessor = VideoPreprocessor(
                apply_clahe=True,
                apply_sharpen=True
            )
            processed_video_path = preprocessor.process(video_file)
            
            # Use processed file for transcription
            avsr_result = get_avsr_confidence(processed_video_path, avsr_model_path)
            
        except Exception as e:
            print(f"   ‚ö† Video preprocessing failed, using original file: {e}")
            avsr_result = get_avsr_confidence(video_file, avsr_model_path)
        finally:
            # Clean up temp file
            if processed_video_path and processed_video_path != video_file and os.path.exists(processed_video_path):
                try:
                    os.remove(processed_video_path)
                except:
                    pass
    else:
        print("üëÅÔ∏èüëÑ Using mock auto_avsr data (no video file provided)")
        avsr_result = {
            "transcript": "the quick brown fox jumps over the lazy dog",
            "overall_confidence": 0.88,
            "word_confidences": [
                ("the", 0.92), ("quick", 0.85), ("brown", 0.88),
                ("fox", 0.90), ("jumps", 0.83), ("over", 0.91),
                ("the", 0.94), ("lazy", 0.82), ("dog", 0.95)
            ]
        }
    
    # Combine word-level confidences
    print("\nüîÑ Combining word-level confidence scores...")
    combined_words = combine_word_confidences(
        deepgram_result["word_confidences"],
        avsr_result["word_confidences"]
    )
    
    for word_info in combined_words:
        agreement_mark = "‚úì" if word_info["agreement"] else "‚úó"
        conf_mark = "‚ö†" if word_info["low_confidence"] else "‚úì"
        print(
            f"   {agreement_mark} {word_info['word']:12s} "
            f"[DG: {word_info['deepgram']['confidence']:.2f}] "
            f"[AVSR: {word_info['avsr']['confidence']:.2f}] "
            f"[Avg: {word_info['average_confidence']:.2f}] {conf_mark}"
        )
    
    # Send to Groq for semantic correction
    print("\n" + "=" * 80)
    groq_result = process_with_groq(
        deepgram_transcript=deepgram_result["transcript"],
        deepgram_confidence=deepgram_result["overall_confidence"],
        deepgram_words=deepgram_result["word_confidences"],
        avsr_transcript=avsr_result["transcript"],
        avsr_confidence=avsr_result["overall_confidence"],
        avsr_words=avsr_result["word_confidences"],
        combined_words=combined_words,
        api_key=groq_api_key,
    )
    
    # Print results
    print("\n" + "=" * 80)
    print(" FINAL RESULTS")
    print("=" * 80)
    print(f"\nAudio (DG):      {deepgram_result['transcript']}")
    print(f"Video (VSR):     {avsr_result['transcript']}")
    print(f"Combined:        {' '.join([w['word'] for w in combined_words])}")
    print(f"Groq Corrected:  {groq_result['corrected_transcript']}")
    print(f"Confidence:      {groq_result['confidence']:.3f}")
    
    if groq_result['corrections']:
        print(f"\nCorrections Applied:")
        for corr in groq_result['corrections']:
            print(f"  ‚Ä¢ '{corr.get('original_phrase', '')}' ‚Üí '{corr.get('corrected_phrase', '')}'")
            if corr.get('reason'):
                print(f"    Reason: {corr['reason']}")
    
    print(f"\nStatus: {groq_result['status']}")
    if groq_result['notes']:
        print(f"Notes: {groq_result['notes']}")
    
    print("=" * 80 + "\n")
    
    # Prepare result dictionary
    result = {
        "deepgram": deepgram_result,
        "avsr": avsr_result,
        "combined_words": combined_words,
        "combined_transcript": ' '.join([w['word'] for w in combined_words]),
        "groq": groq_result,
        "final_transcript": groq_result["corrected_transcript"],
    }
    
    # Log everything
    logger = get_logger()
    logger.log_all(result, video_file=video_file or "mock_data")
    
    return result


if __name__ == "__main__":
    # Example usage with mock data
    results = run_mvp()
    
    print("\n‚úÖ MVP pipeline completed successfully!")
    print(f"\nüìÑ Final Transcript: {results['final_transcript']}")

