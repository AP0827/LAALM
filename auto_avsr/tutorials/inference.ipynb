{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556076dd-8537-4356-8ac3-f4a838b2e896",
   "metadata": {},
   "source": [
    "# AUDIO/VISUAL SPEECH RECOGNITION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27269bb4-af6f-4d93-9141-799400b9131e",
   "metadata": {},
   "source": [
    "This tutorial shows how to use our `InferencePipeline` to perform audio or visual speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e067f7d-cc60-43a0-b65a-7272c9cecc81",
   "metadata": {},
   "source": [
    "**Note** This tutorial requires `mediapipe` or `retinaface` detector. Please refer to [preparation](../preparation#setup) for installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97575fc-bb77-43d0-8eb7-f94baf4cdc89",
   "metadata": {},
   "source": [
    "**Note** To run this tutorial, please make sure you are in tutorials folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa81fb4f-6e89-4238-9f47-3f66eedd3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2fb0de3-af7f-477e-baef-dd595effeef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875bb66-4e96-4c51-85d6-a129748d197a",
   "metadata": {},
   "source": [
    "## 1. Build an inference pipeline\n",
    "\n",
    "The InferencePipeline carries out the following three steps:\n",
    "\n",
    "1. Load audio or video data\n",
    "2. Run pre-processing functions\n",
    "3. Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebfe406-2cd8-4b1a-bf62-32ce1934fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lightning import ModelModule\n",
    "from datamodule.transforms import AudioTransform, VideoTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da3a91c-f515-45f5-8172-ababfd786596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args, _ = parser.parse_known_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac03a1e1-3082-43ca-bb44-e58f96464bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferencePipeline(torch.nn.Module):\n",
    "    def __init__(self, args, ckpt_path, detector=\"retinaface\"):\n",
    "        super(InferencePipeline, self).__init__()\n",
    "        self.modality = args.modality\n",
    "        if self.modality == \"audio\":\n",
    "            self.audio_transform = AudioTransform(subset=\"test\")\n",
    "        elif self.modality == \"video\":\n",
    "            if detector == \"mediapipe\":\n",
    "                from preparation.detectors.mediapipe.detector import LandmarksDetector\n",
    "                from preparation.detectors.mediapipe.video_process import VideoProcess\n",
    "                self.landmarks_detector = LandmarksDetector()\n",
    "                self.video_process = VideoProcess(convert_gray=False)\n",
    "            elif detector == \"retinaface\":\n",
    "                from preparation.detectors.retinaface.detector import LandmarksDetector\n",
    "                from preparation.detectors.retinaface.video_process import VideoProcess\n",
    "                self.landmarks_detector = LandmarksDetector(device=\"cuda:0\")\n",
    "                self.video_process = VideoProcess(convert_gray=False)\n",
    "            self.video_transform = VideoTransform(subset=\"test\")\n",
    "\n",
    "        ckpt = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n",
    "        self.modelmodule = ModelModule(args)\n",
    "        self.modelmodule.model.load_state_dict(ckpt)\n",
    "        self.modelmodule.eval()\n",
    "\n",
    "    def load_video(self, data_filename):\n",
    "        return torchvision.io.read_video(data_filename, pts_unit=\"sec\")[0].numpy()\n",
    "\n",
    "    def forward(self, data_filename):\n",
    "        data_filename = os.path.abspath(data_filename)\n",
    "        assert os.path.isfile(data_filename), f\"data_filename: {data_filename} does not exist.\"\n",
    "\n",
    "        if self.modality == \"audio\":\n",
    "            audio, sample_rate = self.load_audio(data_filename)\n",
    "            audio = self.audio_process(audio, sample_rate)\n",
    "            audio = audio.transpose(1, 0)\n",
    "            audio = self.audio_transform(audio)\n",
    "            with torch.no_grad():\n",
    "                transcript = self.modelmodule(audio)\n",
    "\n",
    "        if self.modality == \"video\":\n",
    "            video = self.load_video(data_filename)\n",
    "            landmarks = self.landmarks_detector(video)\n",
    "            video = self.video_process(video, landmarks)\n",
    "            video = torch.tensor(video)\n",
    "            video = video.permute((0, 3, 1, 2))\n",
    "            video = self.video_transform(video)\n",
    "            with torch.no_grad():\n",
    "                transcript = self.modelmodule(video)\n",
    "\n",
    "        return transcript\n",
    "\n",
    "    def load_audio(self, data_filename):\n",
    "        waveform, sample_rate = torchaudio.load(data_filename, normalize=True)\n",
    "        return waveform, sample_rate\n",
    "\n",
    "    def load_video(self, data_filename):\n",
    "        return torchvision.io.read_video(data_filename, pts_unit=\"sec\")[0].numpy()\n",
    "\n",
    "    def audio_process(self, waveform, sample_rate, target_sample_rate=16000):\n",
    "        if sample_rate != target_sample_rate:\n",
    "            waveform = torchaudio.functional.resample(\n",
    "                waveform, sample_rate, target_sample_rate\n",
    "            )\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa0be0-788d-42b4-8873-0cd7ce804eb3",
   "metadata": {},
   "source": [
    "## 2. Get a video from the samples folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea438ae5-2c1b-420b-a7f2-0a23e3c22ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = \"/home/asish/LAALM/samples/video/bbaf2n.mpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde450d-eeea-4ac3-a651-63a1ddd24258",
   "metadata": {},
   "source": [
    "## 3. VSR inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee0e65-8ef0-4fff-93d3-615b3ad26c77",
   "metadata": {},
   "source": [
    "### 3.1 Download a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc4442d-a112-4822-895c-4faddd8bf808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/asish/LAALM/auto_avsr/pretrained_models/vsr_trlrs2lrs3vox2avsp_base.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf16c2c-5293-4824-9692-167ec18909bd",
   "metadata": {},
   "source": [
    "### 3.2 Initialize VSR pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401c84d6-5910-4959-8761-34cbd1d18c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1766841012.294169 1428160 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1766841012.297280 1428335 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) Graphics (RPL-S)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1766841012.301094 1428160 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1766841012.303216 1428345 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) Graphics (RPL-S)\n"
     ]
    }
   ],
   "source": [
    "setattr(args, 'modality', 'video')\n",
    "pipeline = InferencePipeline(args, model_path, detector=\"mediapipe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde540f5-6bdb-4030-a770-34da73928f39",
   "metadata": {},
   "source": [
    "### 3.3 Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c0fe308-6a0f-4c57-869f-a0114bebdf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asish/LAALM/auto_avsr/.venv/lib/python3.11/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIMBO F2 NOW\n"
     ]
    }
   ],
   "source": [
    "transcript = pipeline(data_filename)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e7193-93fb-4c1b-a5a4-af97eb37d0a1",
   "metadata": {},
   "source": [
    "## 4. ASR inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca5080-e993-43c7-bf93-ec4e367c99ba",
   "metadata": {},
   "source": [
    "### 4.1 Download a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9a7d3-545f-4cb5-8392-341dde7f4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.doc.ic.ac.uk/~pm4115/autoAVSR/asr_trlrs3_base.pth -O ./asr_trlrs3_base.pth\n",
    "model_path = \"./asr_trlrs3_base.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11071d0b-a20e-4486-a3e0-17996ce9dda4",
   "metadata": {},
   "source": [
    "### 4.2 Initialize ASR pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923e7ad-309c-44cd-aa14-d37e83b42714",
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(args, 'modality', 'audio')\n",
    "pipeline = InferencePipeline(args, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef0be9-aef4-4013-af61-12ebbe9c8671",
   "metadata": {},
   "source": [
    "### 4.3 Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332254c8-6c18-40a4-8056-0e896b83adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = pipeline(\"input.mp4\")\n",
    "print(transcript)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
