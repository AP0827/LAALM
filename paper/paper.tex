\documentclass[journal,10pt]{IEEEtran}

\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{microtype}
\usepackage{caption}
\usepackage[hidelinks]{hyperref}
\usepackage{cite}
\usepackage{balance}

\begin{document}

\title{LAALM: A Lip--Audio Aligned Language Model for Noise-Resilient Real-Time Captioning}

\author{
Asish~Kumar~Yeleti,
Aayush~Pandey,
Prof.~Merin~M~Meleet\\
Department of Information Science and Engineering, R V College Of Engineering\\
asishkumary.is23@rvce.edu.in, aayushpandey.is23@rvce.edu.in, merinmeleet@rvce.edu.in
}

\maketitle

% ================= ABSTRACT =================
\begin{abstract}
Automatic Speech Recognition (ASR) systems achieve high accuracy under clean acoustic conditions, yet their performance degrades sharply in realistic environments characterized by background noise, reverberation, and overlapping speakers. Human speech perception addresses these challenges through multimodal integration, exploiting visual cues from lip movements. This paper introduces LAALM, a Lip--Audio Aligned Language Model for noise-resilient real-time captioning. Unlike traditional concatenation-based fusion, LAALM employs a modular "Safe Preprocessing" pipeline that standardizes inputs (25fps, 720p) and applies modality-aware denoising (e.g., FFT-based audio reduction). We propose a novel N-Best Fusion strategy where the top-K visual hypotheses from beam search are semantically aligned with acoustic transcripts using a Large Language Model (LLM). Confidence scores are calibrated using a log-probability exponential mapping to ensure reliable multi-modal weighting. Evaluation on the LRS3 benchmark demonstrates that LAALM significantly outperforms unimodal baselines and naive fusion, particularly in resolving homophones and preserving semantic coherence under low-SNR conditions.
\end{abstract}

\begin{IEEEkeywords}
Audio-visual speech recognition, lip reading, multimodal alignment, cross-attention transformers, noise-robust ASR, language model refinement.
\end{IEEEkeywords}

% ================= INTRODUCTION =================
\section{Introduction}

Automatic Speech Recognition (ASR) has become a core enabling technology for modern human–computer interaction, supporting applications such as voice assistants, automated transcription, live captioning, and assistive communication systems. The last decade has witnessed major advances driven by deep neural architectures and large-scale self-supervised learning, enabling ASR systems to approach human-level performance on clean speech. Models such as wav2vec~2.0 and Whisper demonstrate strong generalization across speakers and domains when acoustic conditions are favorable \cite{baevski2020wav2vec, radford2022whisper}.

Despite these successes, the robustness of ASR systems remains limited in real-world environments. Background noise, reverberation, microphone artifacts, and overlapping speech introduce uncertainty that rapidly degrades recognition accuracy. These limitations are particularly critical in safety-sensitive and accessibility-focused settings such as classrooms, public announcements, broadcasting, and communication aids for the hearing impaired.

Human speech perception provides an instructive contrast. Psycholinguistic and neuroscientific studies show that humans integrate auditory and visual information during speech perception, with lip and facial movements providing crucial disambiguating cues under degraded acoustic conditions. The McGurk effect illustrates that visual speech cues are fundamentally integrated into phoneme perception rather than serving as auxiliary signals. This insight motivates multimodal speech recognition systems that explicitly exploit visual information to improve robustness.

Large-scale datasets such as LRW, LRS2, and LRS3 enabled deep learning–based visual and audiovisual speech recognition in unconstrained settings \cite{chung2016lip, afouras2018lrs3}. Transformer-based architectures further improved temporal modeling and multimodal fusion. However, most audiovisual ASR systems prioritize phonetic alignment and word accuracy, often neglecting higher-level semantic coherence. Under severe noise, outputs may remain syntactically or contextually implausible even when audiovisual alignment is improved.

In parallel, large language models have demonstrated remarkable capabilities in modeling linguistic structure, long-range dependencies, and contextual consistency. Recent work shows that LLMs can correct ASR errors and improve fluency when applied as post-processing modules \cite{li2024asr, ma2024llmec}. However, unconstrained language model integration risks hallucination and semantic drift, especially when acoustic evidence is weak.

This paper proposes LAALM, a Lip--Audio Aligned Language Model that integrates explicit audiovisual alignment with constrained language model–based semantic refinement. By bridging low-level multimodal fusion and high-level linguistic reasoning, LAALM aims to deliver robust, semantically coherent captions suitable for near real-time deployment.

% ================= PROBLEM FORMULATION =================
\section{Problem Formulation}

Let $X_a = \{x_a^1, \ldots, x_a^T\}$ denote an audio signal and $X_v = \{x_v^1, \ldots, x_v^S\}$ denote the corresponding sequence of lip-region video frames. The objective of audiovisual speech transcription is to recover a word sequence $Y = \{y_1, \ldots, y_N\}$ that accurately represents the spoken content conveyed by the speaker.

In contrast to end-to-end audiovisual recognition frameworks that directly model the joint posterior $P(Y \mid X_a, X_v)$, LAALM adopts a modular formulation in which the two modalities are processed independently. Specifically, modality-specific recognizers produce separate hypotheses
\[
Y_a \sim P(Y \mid X_a), \quad Y_v \sim P(Y \mid X_v),
\]
along with associated confidence estimates reflecting modality reliability under prevailing conditions.

In real-world environments, the audio and visual modalities exhibit asymmetric degradation. Acoustic signals are highly sensitive to background noise, reverberation, and channel distortion, while visual speech cues may be affected by occlusion, motion blur, or viewpoint variation. As a result, neither modality can be assumed to be uniformly reliable, and naive fusion strategies that prioritize a single modality often fail under partial corruption.

The central problem addressed by LAALM is therefore the following: given two potentially inconsistent transcription hypotheses $Y_a$ and $Y_v$, each accompanied by uncertainty estimates, determine a final transcription $Y^\ast$ that is both linguistically coherent and maximally supported by the available multimodal evidence. This can be expressed as
\[
Y^\ast = \arg\max_Y \; P(Y \mid Y_a, Y_v, C_a, C_v),
\]
where $C_a$ and $C_v$ denote normalized confidence measures for the audio and visual modalities respectively.

Rather than enforcing explicit temporal or representational alignment between $X_a$ and $X_v$, LAALM resolves cross-modal discrepancies at the semantic level. A language model is employed as a constrained reasoning module that reconciles competing hypotheses, favors higher-confidence evidence, and corrects local inconsistencies while minimizing unwarranted deviations from perceptual inputs. This formulation shifts the emphasis from learned cross-modal correspondence to confidence-aware semantic inference, enabling robust transcription in the presence of modality-specific noise without requiring joint audiovisual training.

% ================= RELATED WORK =================
\section{Related Work}

Research in audiovisual speech recognition (AVSR) has progressed substantially with the availability of large-scale datasets and transformer-based architectures. Early work in visual speech recognition demonstrated that lip movements alone provide sufficient phonetic information for word-level recognition under constrained conditions. The introduction of the LRW dataset marked a significant milestone by enabling large-vocabulary lip reading in unconstrained environments \cite{chung2016lip}. Subsequent datasets such as LRS2 and LRS3 extended this paradigm to continuous, sentence-level audiovisual speech recognition, establishing benchmarks for modern AVSR systems \cite{afouras2018lrs3}.

Initial AVSR approaches relied on early feature concatenation or late fusion of modality-specific predictions. While computationally simple, these strategies suffer from modality imbalance and limited temporal correspondence, particularly when one modality is degraded. Transformer-based architectures addressed these limitations by enabling long-range temporal modeling and cross-modal interactions. Cross-attention mechanisms, in particular, allow one modality to selectively attend to temporally relevant features in another, resulting in improved alignment and robustness \cite{makino2019visual, tsai2019multimodal}.

Recent work has further explored hybrid architectures combining convolutional inductive biases with self-attention. Conformer-based AVSR models demonstrate improved robustness by integrating local temporal modeling with global attention mechanisms \cite{burchi2024fastconformer}. In parallel, self-supervised audiovisual representation learning has emerged as a powerful paradigm. AV-HuBERT jointly models masked audio and visual units to learn modality-invariant representations, significantly improving performance under noisy conditions \cite{shi2022avhubert}. Despite these advances, such models primarily optimize phonetic recognition accuracy and do not explicitly address sentence-level semantic consistency under severe degradation.

Separately, language model–based error correction has gained increasing attention as a means of improving ASR outputs. Recent studies demonstrate that large language models can correct grammatical errors, resolve homophones, and improve fluency when applied as post-editors \cite{li2024asr}. However, these approaches often operate independently of multimodal alignment and may introduce hallucinations or semantic drift when acoustic evidence is weak \cite{ma2024llmec}. 

In contrast to prior work, LAALM integrates explicit audiovisual alignment with constrained language model–based semantic refinement. By grounding linguistic correction in aligned multimodal representations, the proposed framework addresses both phonetic robustness and semantic coherence—two challenges that existing AVSR and LLM-based correction approaches largely treat in isolation.

% ================= METHODOLOGY =================
\section{Methodology}

LAALM is formulated as a modular multimodal transcription framework that integrates independent audiovisual speech recognition with confidence-aware semantic refinement. Unlike end-to-end multimodal learning approaches that rely on jointly optimized latent representations, LAALM is motivated by the practical observation that high-quality pretrained speech recognizers already capture strong modality-specific cues, but remain brittle under modality-specific degradation. The central hypothesis guiding the system design is that robustness can be achieved by deferring multimodal fusion to the decision level and resolving cross-modal inconsistencies through explicit semantic reasoning rather than learned feature alignment.

Accordingly, LAALM separates perceptual inference from semantic reconciliation (Figure \ref{fig:architecture}). This high-level architectural design contrasts with coupled AVSR systems by treating the LLM as an independent reasoner.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{system_architecture.png}
\caption{System Architecture of LAALM. The model consists of independent Audio and Visual encoders feeding into a central Semantic Fusion Engine. The separation of concerns allows for modular upgrades to individual components.}
\label{fig:architecture}
\end{figure}

\subsection{Modality-Specific Speech Recognition}

Given a synchronized audiovisual input consisting of a continuous audio signal $X_a$ and a corresponding video stream $X_v$, LAALM performs parallel inference using modality-specific recognizers.

The audio stream is transcribed using a state-of-the-art neural speech recognition system optimized for robustness across acoustic conditions. Such systems employ large-scale self-supervised acoustic representations and sequence modeling to produce word-level or segment-level transcriptions along with confidence estimates. Let the resulting audio transcript be denoted as
\[
Y_a = \{(w_1, c_1), (w_2, c_2), \ldots, (w_{N_a}, c_{N_a})\},
\]
where $w_i$ represents a predicted token and $c_i \in [0,1]$ denotes its associated confidence score.

In parallel, the visual stream is processed by a visual speech recognition (VSR) model trained in-house on cropped lip-region video sequences. The architecture and training strategy are inspired by recent advances in multilingual visual speech recognition, incorporating spatiotemporal convolutional frontends and sequence modeling to capture articulatory motion patterns. The visual recognizer produces a token sequence
\[
Y_v = \{(u_1, d_1), (u_2, d_2), \ldots, (u_{N_v}, d_{N_v})\},
\]
where $u_j$ denotes a predicted token and $d_j$ represents a confidence measure derived from decoding statistics such as beam probabilities.

Notably, the two recognizers operate independently. The VSR module specifically employs an N-Best Beam Search strategy to capture ambiguous visual phonemes. Instead of a single hypothesis, it generates a set of candidate transcriptions:
\[
\mathcal{Y}_v = \{ (Y_v^{(k)}, d^{(k)}) \}_{k=1}^K
\]
where $K=5$ (beam width). This N-Best list is crucial for resolving homophones (e.g., "date" vs "gate") which are visually indistinguishable but acoustically distinct.

\subsection{Confidence Normalization and Transcript Aggregation}

To enable principled fusion, LAALM maps the raw log-probabilities from the VSR decoder to a calibrated probability space. The confidence score $d^{(k)}$ for the $k$-th hypothesis is computed as:
\[
C_v^{(k)} = \exp\left( \frac{1}{L} \sum_{t=1}^L \log P(u_t^{(k)} | X_v) \right)
\]
This exponential calibration resolves the issue of artificially compressed confidence ranges, providing a realistic measure of uncertainty (0.0--1.0) that is comparable to the acoustic confidence scores.

The normalized outputs are aggregated into a unified transcription bundle:
\[
\mathcal{B} = \{Y_a, \bar{c}_a, Y_v, \bar{c}_v\},
\]
where $\bar{c}_a$ and $\bar{c}_v$ denote global confidence measures for the audio and visual transcripts respectively. This bundle serves as the sole input to the semantic refinement stage.

\subsection{Semantic Alignment via Language Model Reasoning}

Rather than performing feature-level fusion, LAALM resolves cross-modal discrepancies at the linguistic level using a language model as a constrained semantic aligner. Given the aggregated transcription bundle $\mathcal{B}$, the language model is tasked with producing a final transcription $Y^\ast$ that maximizes semantic coherence while remaining grounded in perceptual evidence.

Formally, the refinement process can be expressed as
\[
Y^\ast = \arg\max_Y \; P(Y \mid Y_a, Y_v, \bar{c}_a, \bar{c}_v),
\]
where the conditioning explicitly exposes both modality hypotheses and their associated reliabilities. The language model is prompted to treat the audio transcript as the primary signal under high acoustic confidence, defer to the visual transcript under acoustic degradation, and reconcile disagreements by selecting linguistically plausible alternatives supported by at least one modality.

\subsection{Safe Preprocessing and Differentiated Denoising}

Prior to inference, the pipeline employs a "Safe Preprocessing" module to standardize variance in user-uploaded content.
\begin{enumerate}
    \item \textbf{Standardization}: Video is strictly transcoded to 25 fps and 720p resolution to match the VSR model's training distribution, preventing synchronization drift.
    \item \textbf{Visual Enhancement}: A mild Unsharp Mask (3x3 kernel, 0.5 strength) is applied to define lip boundaries without introducing artifactual edges.
    \item \textbf{Differentiated Denoising}: Audio and video are denoised independently. Audio undergoes FFT-based reduction (\texttt{afftdn}) for stationary noise removal, while video utilizes a 3D spatiotemporal filter (\texttt{hqdn3d}) to preserve temporal consistency in facial movements.
\end{enumerate}
This decoupled preprocessing ensures that degradations in one modality do not propagate to the other before inference begins.

\subsection{System Characteristics and Design Implications}

The proposed methodology reflects a deliberate shift from representation learning to system-level reasoning. By avoiding joint audiovisual training and cross-modal attention mechanisms, LAALM prioritizes modularity, interpretability, and deployment feasibility. Each component can be improved independently, allowing advances in acoustic ASR, visual speech recognition, or language modeling to be incorporated without architectural redesign.

While the system does not explicitly learn cross-modal alignments, it follows a strict inferential workflow (Figure \ref{fig:workflow}). This approach is particularly well-suited to real-world scenarios characterized by asymmetric modality reliability, such as noisy environments or partial visual occlusion.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{workflow.png}
\caption{Inference Workflow. The pipeline decouples functional standardization (Safe Preprocessing) from aesthetic enhancement. Parallel audio and visual streams are fused using a calibrated N-Best strategy mediated by an LLM.}
\label{fig:workflow}
\end{figure}


% ================= DATASETS =================
\section{Datasets and Preprocessing}

LAALM is evaluated using established audiovisual speech datasets that support both visual speech recognition and multimodal benchmarking under varying acoustic conditions. The system design distinguishes between datasets used for training the visual speech recognizer and those used for end-to-end evaluation of multimodal transcription robustness.

\subsection{Visual Speech Recognition Datasets}

The visual speech recognition component is trained using publicly available large-scale lip-reading datasets. Sentence-level visual speech data is sourced from the LRS3 dataset, which provides unconstrained, naturalistic speech with significant variability in pose, lighting, and speaking style \cite{afouras2018lrs3}. To improve lexical discrimination and stabilize early-stage training, word-level supervision from the LRW dataset is used for pretraining \cite{chung2016lip}. Figure \ref{fig:dataset-examples} illustrates representative samples processed by our pipeline.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{dataset_samples.png}
\caption{Multimodal input samples from the LRS3 dataset showing the 88x88 mouth ROI crops extracted by the visual frontend and their corresponding audio waveforms. The variability in lip morphology and lighting underscores the need for robust visual encoding.}
\label{fig:dataset-examples}
\end{figure}

Video preprocessing follows standard visual speech recognition pipelines. Face detection is applied to each frame, followed by mouth-region cropping based on facial landmarks. The cropped lip regions are resized to a fixed spatial resolution and normalized to reduce illumination variance. Temporal sampling is performed at a constant frame rate to preserve articulatory motion continuity. No explicit alignment with audio features is enforced during training, as the visual model operates independently.

\subsection{Audio Transcription Data}

Audio transcriptions are generated using a high-performance neural speech recognition system optimized for robustness across diverse acoustic environments. As the audio model is not trained within the LAALM framework, no dedicated audio training dataset is required. Instead, audio streams associated with evaluation videos are passed directly to the transcription system at inference time.

To assess robustness under degraded conditions, controlled noise perturbations are applied to evaluation audio using standard noise corpora and signal-to-noise ratio (SNR) scaling. This setup simulates real-world scenarios such as background chatter and environmental noise, allowing systematic analysis of modality reliability asymmetry.

\subsection{Multimodal Evaluation Setup}

For evaluation, synchronized audiovisual samples are drawn from sentence-level datasets, ensuring that both modalities correspond to the same spoken content. Unlike end-to-end multimodal learning approaches, LAALM does not require frame-level temporal alignment between audio and video. Synchronization is used solely to guarantee semantic correspondence at the utterance level.

During inference, audio and visual streams are processed independently to produce modality-specific transcripts and confidence estimates. These outputs form the basis for downstream confidence-aware semantic alignment, rather than serving as inputs to learned fusion modules. This design choice reflects the system’s emphasis on modularity and deployment realism.


\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{speech_sample.png}
\caption{Preprocessing and ROI Extraction. Left: Original input frame (variable quality). Center: Output of the "Safe Preprocessing" module (Standardized to 25fps/720p with mild unsharp masking and denoising). Right: The 88x88 grayscale mouth-region crop extracted by the VSR frontend, serving as the input to the visual encoder.}
\label{fig:dataset-sample}
\end{figure}


% ================= TRAINING =================
\section{Training and Evaluation Protocol}

The LAALM framework follows a modular training and evaluation protocol in which modality-specific components are optimized independently and integrated at inference time. This design reflects the system’s emphasis on deployment realism and avoids the need for joint audiovisual retraining.

\subsection{Visual Speech Recognition Training}

The visual speech recognition model is trained in-house using supervised learning on lip-region video data. Optimization is performed using the AdamW optimizer with scheduled learning rate decay and mixed-precision training for computational efficiency. Training proceeds in two stages: an initial pretraining phase on word-level data to stabilize visual feature extraction, followed by fine-tuning on sentence-level utterances to capture longer temporal dependencies.

To improve robustness, curriculum learning is applied by progressively introducing visual perturbations such as motion blur, partial occlusion, and reduced spatial resolution. These augmentations simulate realistic visual degradation while preserving linguistic content. No audio information is used during visual model training.

\subsection{Audio Transcription and Inference}

The audio transcription component is treated as a fixed, external inference system and is not trained as part of LAALM. Audio streams are transcribed directly at evaluation time, producing token-level outputs and associated confidence estimates. To assess robustness under adverse conditions, controlled noise is injected into evaluation audio at varying signal-to-noise ratios (SNRs). This setup enables systematic comparison between audio-only and multimodal configurations without modifying the underlying acoustic model.

\subsection{Semantic Alignment Evaluation}

The language model–based semantic alignment module is evaluated solely at inference time. Given audio and visual transcripts along with normalized confidence scores, the language model produces a refined transcription under constrained decoding. No gradient-based optimization is performed for this stage. Evaluation focuses on the effectiveness of semantic reconciliation rather than generative language modeling performance.

\subsection{Metrics and Baselines}

Performance is measured using Word Error Rate (WER) and Character Error Rate (CER) across multiple SNR levels. Latency and real-time factor are additionally reported to assess feasibility for real-time or near–real-time deployment.

Baselines include:
\begin{itemize}
    \item Audio-only speech recognition.
    \item Visual-only speech recognition.
    \item A naive late-fusion baseline that selects the transcript with higher confidence without semantic reconciliation.
\end{itemize}



% ================= RESULTS =================
\section{Results and Analysis}

We evaluate LAALM on the LRS3 test set, simulating realistic deployment conditions by injecting non-stationary noise (Cafe/Street) at varying Signal-to-Noise Ratios (SNR).

\subsection{Quantitative Performance}

Table \ref{tab:results} presents the Word Error Rate (WER) comparison across four noise regimes. In clean conditions ($>20$dB), the audio-only baseline (DeepGram Nova-2) achieves near-perfect transcription (2.0\% WER). However, performance degrades sharply at 0dB (18.4\% WER). LAALM demonstrates superior robustness, maintaining a WER of 8.9\% even at 0dB, a relative improvement of 51.6\% over audio-only inference.

The naive fusion baseline, which selects the highest-confidence unimodal transcript, suffers from "false certainty" in the visual modality, often selecting plausible but incorrect lip-readings (28.4\% WER at -5dB). LAALM's calibrated N-Best fusion mitigates this, achieving 14.2\% WER in the most severe noise condition.

This robustness arises from the \textbf{Orthogonality of Error Modes}. Audio errors are typically caused by additive spectral masking, whereas visual errors stem from the ambiguity of many-to-one viseme mappings (e.g., /p/ vs /b/). Since these error distributions are statistically independent, the joint entropy $H(Y | X_a, X_v)$ is significantly lower than either conditional entropy in isolation. LAALM effectively treats the visual stream as a "complementary filter" that remains informative precisely when spectral cues are corrupted.


\begin{table}[h]
\centering
\caption{WER comparison across varying SNR levels on the LRS3 dataset.}
\label{tab:results}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{System / SNR} & \textbf{Clean} & \textbf{10dB} & \textbf{0dB} & \textbf{-5dB} \\
\hline
Audio-only (DeepGram) & \textbf{2.0} & 4.5 & 18.4 & 42.1 \\
Visual-only (Auto-AVSR) & 20.3 & 20.3 & 20.5 & 21.2 \\
Naive Late Fusion & 2.0 & 4.2 & 15.7 & 28.4 \\
\hline
\textbf{Proposed LAALM} & 2.1 & \textbf{3.8} & \textbf{8.9} & \textbf{14.2} \\
\hline
\end{tabular}
\end{table}

\subsection{Ablation Studies}

We quantify the impact of key architectural decisions in Table \ref{tab:ablation}.

\textbf{Impact of Safe Preprocessing}: Disabling the standardization module leads to a 1.5\% increase in WER, primarily due to synchronization drift in videos not native to 25fps. The "aesthetic" enhancement filters contribute marginally to accuracy (0.2\%) but significantly improve user perceived quality.

\textbf{Impact of N-Best Fusion}: Switching from Top-1 to Top-5 Beam Search provides the largest single gain (-3.2\% WER). Theoretically, this demonstrates \textbf{Distributional Recovery}: standard Top-1 decoding collapses the posterior $P(Y | X_v)$ to a point estimate, discarding uncertainty structure. Forwarding the top-$K$ hypotheses preserves the local probability manifold, allowing the LLM to access the true hypothesis even when it is locally suboptimal (e.g., ranked 3rd).

\textbf{Impact of Calibration}: The 1.6\% gain from calibration underscores the importance of \textbf{Uncertainty Alignment}. VSR logits are not naturally probability measures; without the proposed exponential mapping, the fusion logic is dominated by the modality with higher variance (typically Video), leading to "over-trust" in visual hallucinations.

\begin{table}[h]
\centering
\caption{Ablation study on component contributions (at 0dB SNR).}
\label{tab:ablation}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Configuration} & \textbf{WER (\%)} & \textbf{$\Delta$} \\
\hline
Full LAALM & 8.9 & - \\
\hline
w/o N-Best Fusion (Top-1 only) & 12.1 & +3.2 \\
w/o Calibrated Confidence & 10.5 & +1.6 \\
w/o Safe Preprocessing (Std.) & 10.4 & +1.5 \\
w/o Differentiated Denoising & 9.4 & +0.5 \\
\hline
\end{tabular}
\end{table}

\subsection{Qualitative Analysis: Homophone Resolution}

A key advantage of LAALM is its ability to resolve phonetic ambiguities. Consider the utterance: \textit{"We must close the \textbf{gate}."}
\begin{itemize}
    \item \textbf{Audio (Noisy)}: \textit{"We must close the ... [noise]"} (Confidence: 0.1)
    \item \textbf{Video Top-1}: \textit{"We must close the \textbf{date}."} (Visually identical homophone)
    \item \textbf{Video Top-3}: \{\textit{date}, \textit{late}, \textit{gate}\}
\end{itemize}
The semantic fusion engine, prompted with the N-Best list and the context of "closing", correctly selects "gate" despite it being the 3rd visual candidate. This reasoning capability is absent in naive fusion approaches.

\subsection{Real-World Latency}
The modular architecture introduces a latency of 480ms (post-utterance), comprised of: Audio Inference (50ms), Visual Inference (280ms), and LLM Fusion (150ms). This falls within the acceptable range for near-real-time captioning ($<1$s).

\subsection{Novel Contributions}
This work explicitly contributes:
\begin{enumerate}
    \item \textbf{Decoupled Standardization Framework}: A robust preprocessing strategy that enforces VSR stability (25fps/720p) independent of optional aesthetic enhancements.
    \item \textbf{Calibrated N-Best Fusion}: A novel integration of VSR beam-search candidates with LLM-based semantic reasoning, outperforming Top-1 fusion by 3.2\% WER.
    \item \textbf{Differentiated Denoising}: A dual-pipeline approach applying signal-specific filters (\texttt{afftdn} vs \texttt{hqdn3d}) to maximize multimodal SNR.
\end{enumerate}

% ================= DISCUSSION =================
\section{Limitations and Discussion}

While LAALM establishes a robust baseline for modular audiovisual application, several limitations warrant discussion.

\textbf{Visual Dependency \& Occlusion}: The system's resilience is predicated on the availability of a clear frontal view of the speaker. Extreme head poses ($>45^{\circ}$ yaw), poor illumination ($<50$ lux), or transient occlusions (e.g., hand-over-mouth gestures) significantly degrade VSR confidence. Although the confidence calibration module effectively down-weights unreliable visual cues, in these scenarios the system reverts to audio-only performance, negating the multimodal advantage.

\textbf{Computational Overhead}: The decoupled architecture, while flexible, incurs a higher computational cost than end-to-end models. Running two distinct inference engines (Audio ASR and Visual VSR) plus a Large Language Model for fusion results in a threefold increase in FLOPS compared to unimodal baselines. Optimization techniques such as model quantization and speculative decoding are critical directions for future work to reduce the 480ms latency.

\textbf{Semantic Hallucination Risks}: Despite the constrained promoting strategy, LLMs inherently risk "over-smoothing" rare proper nouns or technical jargon to more common vocabulary. Our N-Best injection mitigates this, but purely semantic errors remain a challenge, particularly in low-resource specialized domains (e.g., medical transcription).

\textbf{Speaker Diarization}: The current implementation assumes a single active speaker. Overlapping speech in multi-speaker scenarios poses a challenge for the visual frontend, which tracks a single face. Integrating audio-visual speaker separation (AV-Sep) would be a necessary extension for meeting transcription.

% ================= CONCLUSION =================
\section{Conclusion}

This paper presented LAALM, a Lip--Audio Aligned Language Model that redefines robust speech recognition through the lens of semantic consistency rather than just feature alignment. By decoupling standard preprocessing from aesthetic enhancement, and employing a novel N-Best fusion strategy mediated by a Large Language Model, LAALM addresses the critical failure modes of conventional ASR in noisy environments.

Our results demonstrate that the "semantic repair" paradigm—leveraging the reasoning capabilities of LLMs to reconcile conflicting perceptual inputs—is a viable alternative to complex end-to-end joint training. We achieved a 51.6\% relative reduction in WER at 0dB SNR, proving that visual cues, when correctly calibrated and semantically aligned, are indispensable for reliable communication in the wild. Future work will focus on end-to-end differentiable optimization of the fusion prompt and reducing the latency footprint for embodied AI applications.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{99}

\bibitem{chung2016lip}
J. S. Chung and A. Zisserman, ``Lip Reading in the Wild,'' BMVC, 2016.

\bibitem{afouras2018lrs3}
T. Afouras, J. S. Chung, and A. Zisserman, ``LRS3-TED: A Large-Scale Dataset for Visual Speech Recognition,'' arXiv:1809.00496, 2018.

\bibitem{makino2019visual}
T. Makino et al., ``Visual Speech Recognition Using Deep Learning,'' IEICE Transactions, 2019.

\bibitem{tsai2019multimodal}
Y.-H. H. Tsai et al., ``Multimodal Transformer for Unaligned Multimodal Language Sequences,'' ACL, 2019.

\bibitem{shi2022avhubert}
B. Shi et al., ``Learning Audio-Visual Speech Representation by Masked Multimodal Clusters,'' ICLR, 2022.

\bibitem{burchi2024fastconformer}
M. Burchi et al., ``Multilingual Audio-Visual Speech Recognition with Hybrid CTC/RNN-T Fast Conformer,'' arXiv:2405.12983, 2024.

\bibitem{baevski2020wav2vec}
A. Baevski et al., ``wav2vec 2.0,'' NeurIPS, 2020.

\bibitem{radford2022whisper}
A. Radford et al., ``Robust Speech Recognition via Large-Scale Weak Supervision,'' OpenAI, 2022.

\bibitem{li2024asr}
S. Li et al., ``Investigating ASR Error Correction with Large Language Models,'' Interspeech, 2024.

\bibitem{ma2024llmec}
R. Ma et al., ``Large Language Models for ASR Error Correction,'' arXiv:2409.09554, 2024.

\bibitem{wu2016deep}
D. Wu, L. Pigou, P.-J. Kindermans, N. D.-H. Le, L. Shao, J. Dambre, and J.-M. Odobez, ``Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and Recognition,'' \textit{IEEE TPAMI}, vol. 38, no. 8, pp. 1583--1597, 2016.

\bibitem{ren2020comprehensive}
H. Ren, Y. Hu, and H. Zhao, ``A Comprehensive Survey of Audio-Visual Speech Recognition,'' \textit{IEEE Access}, 2021.

\bibitem{petridis2018end}
S. Petridis, T. Stafylakis, P. Ma, et al., ``End-to-end Audiovisual Speech Recognition,'' \textit{IEEE ICASSP}, 2018.

\end{thebibliography}

\end{document}
