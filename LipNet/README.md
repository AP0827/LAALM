# LipNet: End-to-End Sentence-level Lipreading

This directory contains the **LipNet visual speech recognition component** used within the LAALM multi-modal system.

> **Note**: LipNet is one component of the larger LAALM system. For the complete multi-modal pipeline combining audio + visual + LLM correction, see [../README.md](../../README.md).

![LipNet performing prediction](../../assets/lipreading.gif)

---

## ğŸ“š What is LipNet?

LipNet is a Keras/TensorFlow implementation of the paper:
> **LipNet: End-to-End Sentence-level Lipreading**  
> Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, Nando de Freitas  
> https://arxiv.org/abs/1611.01599

It performs **visual speech recognition** - understanding what words are being spoken by analyzing lip movements in video.

---

## ğŸ“Š Performance Results

| Scenario | Epoch | CER | WER | BLEU |
|----------|:-----:|:-----:|:-----:|:-----:|
| Unseen speakers | 178 | 6.19% | 14.19% | 88.21% |
| Overlapped speakers | 368 | 1.56% | 3.38% | 96.93% |

**Legend:**
- **CER**: Character Error Rate (lower is better)
- **WER**: Word Error Rate (lower is better)
- **BLEU**: BLEU Score (higher is better)

---

## ğŸ“ Directory Structure

```
models/lipnet/
â”œâ”€â”€ lipnet/                              # LipNet implementation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                         # Neural network model
â”‚   â”œâ”€â”€ model2.py                        # Alternative model
â”‚   â”œâ”€â”€ core/                            # Core components
â”‚   â”‚   â”œâ”€â”€ decoders.py                  # CTC decoders
â”‚   â”‚   â”œâ”€â”€ layers.py                    # Custom layers
â”‚   â”‚   â””â”€â”€ loss.py                      # Loss functions
â”‚   â”œâ”€â”€ helpers/                         # Utility helpers
â”‚   â”‚   â”œâ”€â”€ list.py
â”‚   â”‚   â””â”€â”€ threadsafe.py
â”‚   â”œâ”€â”€ lipreading/                      # Lipreading-specific
â”‚   â”‚   â”œâ”€â”€ aligns.py
â”‚   â”‚   â”œâ”€â”€ callbacks.py
â”‚   â”‚   â”œâ”€â”€ curriculums.py
â”‚   â”‚   â”œâ”€â”€ generators.py
â”‚   â”‚   â”œâ”€â”€ helpers.py
â”‚   â”‚   â”œâ”€â”€ videos.py
â”‚   â”‚   â””â”€â”€ visualization.py
â”‚   â””â”€â”€ utils/                           # Utilities
â”‚       â”œâ”€â”€ spell.py                     # Spelling correction
â”‚       â””â”€â”€ wer.py                       # WER computation
â”‚
â”œâ”€â”€ training/                            # Training scenarios
â”‚   â”œâ”€â”€ unseen_speakers/                 # Training on unseen speakers
â”‚   â”œâ”€â”€ unseen_speakers_curriculum/      # With curriculum learning
â”‚   â”œâ”€â”€ overlapped_speakers/             # Training on overlapped speakers
â”‚   â”œâ”€â”€ overlapped_speakers_curriculum/  # With curriculum learning
â”‚   â””â”€â”€ random_split/
â”‚
â”œâ”€â”€ evaluation/                          # Evaluation utilities
â”‚   â”œâ”€â”€ predict.py                       # Single file prediction
â”‚   â”œâ”€â”€ predict_batch.py                 # Batch prediction
â”‚   â”œâ”€â”€ confusion.py                     # Confusion matrices
â”‚   â”œâ”€â”€ saliency.py                      # Saliency maps
â”‚   â”œâ”€â”€ stats.py                         # Statistics
â”‚   â”œâ”€â”€ phonemes.txt                     # Phoneme list
â”‚   â””â”€â”€ models/                          # Pre-trained weights
â”‚       â”œâ”€â”€ unseen-weights178.h5         # Unseen speakers model
â”‚       â””â”€â”€ overlapped-weights368.h5     # Overlapped speakers model
â”‚
â””â”€â”€ samples/                             # Sample videos for testing
    â”œâ”€â”€ GRID/
    â”‚   â”œâ”€â”€ bbaf2n.mpg
    â”‚   â”œâ”€â”€ brbk7n.mpg
    â”‚   â””â”€â”€ ... (more samples)
    â””â”€â”€ bbaf2n/
        â””â”€â”€ (example speaker samples)
```

---

## ğŸ”§ Dependencies

```
Keras 2.0+
TensorFlow 1.0+ (or 2.x with compatibility)
NumPy
OpenCV (for video processing)
Matplotlib (for visualization)
```

See [../../requirements.txt](../../requirements.txt) for complete dependency list.

---

## âš¡ Quick Start: Using Pre-trained Weights

### 1. Installation
```bash
cd /path/to/LAALM
pip install -e models/lipnet/
```

### 2. Load Pre-trained Model
```python
from lipnet.model import LipNet
from lipnet.helpers import get_preprocessing_from_env

# Load pre-trained weights
model = LipNet()
model.load_weights('models/lipnet/evaluation/models/unseen-weights178.h5')

# Get video
video_path = 'models/lipnet/samples/GRID/bbaf2n.mpg'
```

### 3. Make Predictions
```python
from lipnet.lipreading.videos import get_frames_from_video

# Load and preprocess video
frames = get_frames_from_video(video_path)

# Get predictions
prediction = model.predict(frames[None, :, :, :, :])
print(f"Predicted: {prediction}")
```

---

## ğŸ“ Training Custom Models

### Training Scenarios Available

#### 1. Unseen Speakers
Train on specific speakers, test on completely new speakers.

```bash
cd training/unseen_speakers/
python train.py
```

#### 2. Overlapped Speakers
Train on overlapped speech (multiple speakers simultaneously).

```bash
cd training/overlapped_speakers/
python train.py
```

#### 3. Curriculum Learning
Train with gradually increasing difficulty levels.

```bash
cd training/unseen_speakers_curriculum/
python train.py
```

### Custom Training
```python
from lipnet.model import LipNet
from lipnet.lipreading.generators import DataGenerator

# Create model
model = LipNet()

# Create data generators
train_gen = DataGenerator('path/to/train/data')
val_gen = DataGenerator('path/to/val/data')

# Train
model.fit_generator(
    train_gen,
    validation_data=val_gen,
    epochs=100,
    callbacks=[...],
    verbose=1
)
```

---

## ğŸ“Š Model Architecture

The LipNet model includes:

1. **Temporal Convolutional Layers**: Extract spatial-temporal features from video frames
2. **Bidirectional LSTM**: Capture long-range dependencies in sequences
3. **CTC Loss**: Connectionist Temporal Classification for sequence-to-sequence learning
4. **Beam Search Decoder**: Find most likely word sequences

```
Input Video (batch_size, frames, height, width, channels)
    â†“
Conv3D Layers (Spatial-temporal feature extraction)
    â†“
Bidirectional LSTM (Sequence modeling)
    â†“
Dense Layer (Classification)
    â†“
CTC Decoding (Convert to words)
    â†“
Output (Predicted words)
```

---

## ğŸ“ˆ Evaluation Metrics

### Evaluation Scripts
```bash
# Single file prediction
cd evaluation/
python predict.py ../samples/GRID/bbaf2n.mpg

# Batch prediction
python predict_batch.py ../samples/GRID/

# Confusion matrix analysis
python confusion.py

# WER (Word Error Rate) computation
python stats.py --reference file.txt --hypothesis output.txt
```

### Supported Metrics
- **CER**: Character Error Rate
- **WER**: Word Error Rate
- **BLEU**: BLEU Score (borrowed from MT evaluation)
- **Confusion Matrix**: Per-character/word confusion analysis
- **Saliency Maps**: Visualization of attention regions

---

## ğŸ¬ Dataset: GRID Corpus

This model is trained on the **GRID Corpus**:
- **Size**: 34 speakers, 1,000 sentences each
- **Format**: Video + aligned transcripts
- **URL**: http://spandh.dcs.shef.ac.uk/gridcorpus/

Sample sentence structure:
```
"Place red at A-two now" (syntax: <command> <color> <preposition> <letter> <digit> <adverb>)
```

---

## ğŸ” Key Features

### 1. Curriculum Learning
Gradually increase task difficulty:
- Start with limited vocabulary
- Progress to full GRID sentences
- Adaptive difficulty based on performance

### 2. Bidirectional LSTM
Capture context from both directions:
- Forward pass: look ahead
- Backward pass: look behind
- Combine for better understanding

### 3. CTC Decoding
Handle variable-length sequences:
- No forced alignment needed
- Automatic audio/text synchronization
- Supports greedy and beam search

### 4. Data Augmentation
Improve robustness:
- Frame dropping
- Noise injection
- Geometric transformations

---

## ğŸš€ Integration with LAALM

Within the LAALM system, LipNet is used as follows:

```python
from models.lipnet.lipnet.model import LipNet

# Load LipNet
lipnet_model = LipNet()
lipnet_model.load_weights('models/lipnet/evaluation/models/unseen-weights178.h5')

# Get LipNet predictions
lipnet_output = lipnet_model.predict(video_frames)
lipnet_confidence = compute_confidence(lipnet_output)

# Feed into LAALM pipeline
from Transformer import TransformerPipeline

pipeline = TransformerPipeline()
result = pipeline.process(
    deepgram_transcript=audio_transcript,
    deepgram_confidence=0.92,
    lipnet_transcript=lipnet_output,
    lipnet_confidence=lipnet_confidence
)
```

---

## ğŸ”— Related Resources

- **Original Paper**: https://arxiv.org/abs/1611.01599
- **GitHub**: https://github.com/rizkiarm/LipNet
- **GRID Corpus**: http://spandh.dcs.shef.ac.uk/gridcorpus/
- **TensorFlow/Keras**: https://www.tensorflow.org/

---

## ğŸ“ Citation

If you use LipNet in your research, please cite:

```bibtex
@inproceedings{assael2016lipnet,
  title={LipNet: End-to-End Sentence-level Lipreading},
  author={Assael, Yannis M and Shillingford, Brendan and Whiteson, Shimon and De Freitas, Nando},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
```

---

## âš ï¸ Known Limitations

1. **GRID-specific**: Trained primarily on GRID corpus (controlled vocabulary, limited domains)
2. **Frontal view**: Works best with frontal face video
3. **Frame rate**: Optimized for 25 FPS video
4. **English only**: Trained on English speech
5. **Synchronized audio**: Timing must align with video

---

## ğŸ†˜ Troubleshooting

### Model Loading Issues
```python
# Use explicit backend
import os
os.environ['KERAS_BACKEND'] = 'tensorflow'

from lipnet.model import LipNet
```

### Out of Memory
```python
# Reduce batch size
model.fit_generator(
    generator,
    batch_size=4,  # Instead of 32
)
```

### Video Processing Errors
```python
# Check video format
import cv2
cap = cv2.VideoCapture('your_video.mpg')
print(f"FPS: {cap.get(cv2.CAP_PROP_FPS)}")
print(f"Frames: {int(cap.get(cv2.CAP_PROP_FRAME_COUNT))}")
```

---

**Last Updated**: 2024  
**Component Status**: Stable âœ…  
**Maintenance**: LAALM Project

For multi-modal pipeline features and integration help, see [../../README.md](../../README.md)
